buildscript{
    repositories {
        jcenter()
    }
    dependencies{
        classpath 'com.github.jengelman.gradle.plugins:shadow:1.2.4'
    }
}

subprojects{
    apply plugin: 'java'
    sourceCompatibility = 1.7

    repositories {
        jcenter()
    }

    dependencies {
        compileOnly "org.apache.hadoop:hadoop-client:${PROJECT_HADOOP_VERSION}"
        compile 'com.google.code.gson:gson:+'
    }

    if (project.name != 'Shared') {
        apply plugin: 'com.github.johnrengelman.shadow'
        dependencies {
            compile project(":Shared")
        }
    }
}

ext{
    DATASET_DIR = 'DatasetFiles'
    OUTPUT_JAR = "build/libs/${rootProject.name}-all.jar"
}

task startHadoop(){
    doLast{
        exec{
            commandLine 'sh', "${HADOOP_HOME}/sbin/start-dfs.sh"
        }
        exec{
            commandLine 'sh', "${HADOOP_HOME}/sbin/start-yarn.sh"
        }
    }
}

task stopHadoop(){
    doLast{
        exec{
            commandLine 'sh', "${HADOOP_HOME}/sbin/stop-yarn.sh"
        }
        exec{
            commandLine 'sh', "${HADOOP_HOME}/sbin/stop-dfs.sh"
        }
    }
}

task uploadDataFiles(){
    doLast{
        try{
            exec{
                commandLine "${HADOOP_HOME}/bin/hdfs", 'dfs', '-mkdir', '-p', PROJECT_HDFS_INPUT_DIR
            }
        }catch (Exception ignore){}

        FileTree tree = fileTree(dir: DATASET_DIR, include: "*")
        tree.each {File file ->
            exec{
                commandLine "${HADOOP_HOME}/bin/hdfs", 'dfs', '-copyFromLocal', '-f', file.path, PROJECT_HDFS_INPUT_DIR
            }
        }
    }
}
